{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a26b63",
   "metadata": {},
   "source": [
    "# Section 0: Preamble <a id=preamble></a>\n",
    "<a id=preamble></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f8554",
   "metadata": {},
   "source": [
    "## Project:\n",
    "Description: This IPYNB script is part of my internship project. The project repository is available on GitHub. <br>\n",
    "Data: Data is not available because of data sharing restrictions.<br>\n",
    "Date: 2023-01-01 (start)<br>\n",
    "\n",
    "## Versions:\n",
    "Apple M2 macOS \n",
    "Pandas: 1.4.4<br>\n",
    "Numpy: 1.21.5<br>\n",
    "Matplotlib: 3.5.2<br>\n",
    "Seaborn: 0.11.2<br>\n",
    "scikit-learn: 1.1.1<br>\n",
    "SciPy: 1.9.1<br>\n",
    "PG: 0.5.3<br>\n",
    "\n",
    "## Author:\n",
    "**Name: Ekin Derdiyok<br>\n",
    "GitHub: https://github.com/ekinderdiyok<br>\n",
    "LinkedIn: https://www.linkedin.com/in/ekinderdiyok/<br>\n",
    "Email: [ekin.derdiyok@icloud.com](mailto:ekin.derdiyok@icloud.com)<br>\n",
    "\n",
    "## Table of Contents:\n",
    "Section 0: [Preamble](#preamble)<br>\n",
    "Section 1: [Import](#import-install)<br>\n",
    "Section 2: [Explore](#explore)<br>\n",
    "Section 3: [Data Wrangling](#data-wrangling)<br>\n",
    "Section 4: [Tweet Analysis](#tweet-analysis)<br>\n",
    "Section 5: [User Analysis](#user-analysis)<br>\n",
    "Section 6: [Attrition Bias](#attrition-bias)<br>\n",
    "Section 7: [Clustering](#clustering)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Section 7.1: [K-Means Clustering](#k-means-clustering)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Section 7.2: [t-SNE](#t-sne)<br>\n",
    "Section 8: [Multiple Linear Regression](#multiple-linear-regression)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd780d",
   "metadata": {},
   "source": [
    "# Section 1: Data import and package installation\n",
    "<a id=import-install></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2405288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pingouin\n",
    "pip install pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import pingouin as pg\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the import is succesful and for documentation\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(sns.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(scipy.__version__)\n",
    "print(pg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"/Users/ekinderdiyok/Documents/MPI/Twitter/public_dataset.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to exclude tweets with 0 favs or RTs\n",
    "data[\"engagement\"] = data.favorite_count + data.retweet_count # Create a new variable \n",
    "(data.engagement == 0).sum() # count the number of tweets with 0 engagement\n",
    "\n",
    "data = data.loc[data.engagement>0,:] # do the subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b00cb1",
   "metadata": {},
   "source": [
    "## Section 2: Explore\n",
    "<a id=explore></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73128357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment in the data\n",
    "# * Very rough way to look at the sentiments in the data. Simply summing up the sentiment scores that are the number of occurences of sentimental words across all tweets.\n",
    "\n",
    "_=data.loc[:,\"sentiment_anger\":\"sentiment_positive\"].sum().sort_values(ascending=False).plot(kind=\"bar\") # Draws a barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment in the data\n",
    "# Very rough way to look at the motives in the data. Simply summing up the motive scores that are the self-reported in a 6-point likert scale\n",
    "\n",
    "_=data.loc[:,\"motive_entertain\":\"motive_informothers\"].sum().sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774fc55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of engagement percentage\n",
    "# Exploring the engagement percentage to better understand the data, not part of the report.\n",
    "# Engagement percentage is defined as (retweet count + favorite count)/follower count of the sender * 100\n",
    "\n",
    "data[\"pct_eng\"] = data.engagement / data.followers_count * 100\n",
    "data.loc[data[\"pct_eng\"] > 100000,\"pct_eng\"] = np.nan\n",
    "data[\"pct_eng\"].hist()\n",
    "plt.yscale(\"log\")\n",
    "#plt.ylim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ba82c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation between sentiments and motives\n",
    "# * One important finding was to add tweet length as a covariate which reduces down the inflated correlation between sentiments. Since dictionary methods counts the occurence of words, and longer tweets tend to have more words some of which are sentimental, correlation among sentiments were inflated.\n",
    "# * Due to space restrictions and not being directly related to other analysis,is not present in the report.\n",
    "# * Adding text width as covariance reduces the correlation between sentiment_positive and sentiment_negative\n",
    "\n",
    "data[\"mood\"] = data.sentiment_positive - data.sentiment_negative # Create a new column that nullifies the tweets with equal amount of positive and negative words\n",
    "sents_mots = [['sentiment_anger', 'sentiment_disgust', 'sentiment_fear', # Create a list of cols for correlation table\n",
    "       'sentiment_anticipation', 'sentiment_joy', 'sentiment_sadness',\n",
    "       'sentiment_surprise', 'sentiment_trust', 'sentiment_negative',\n",
    "       'sentiment_positive',\"mood\"],['motive_provoke', 'motive_savecontent', 'motive_showemotions',\n",
    "       'motive_connectwothers', 'motive_showachievement',\n",
    "       'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention',\n",
    "       'motive_provepoint', 'motive_causechaos', 'motive_bringattention',\n",
    "       'motive_influence', 'motive_surpriseothers', 'motive_informothers']]\n",
    "\n",
    "corr = pg.pairwise_corr(data=data, columns=sents_mots) # sentiment_positive and sentiment_negative are correlated, so add display_text_width as covariance\n",
    "sig_corr_table = corr[corr[\"p-unc\"]<0.001] # Filter for p < .001\n",
    "sig_corr_table[\"abs_r\"] = abs(sig_corr_table.r) # Create abs value column for r\n",
    "display(sig_corr_table.sort_values(by=\"abs_r\",ascending=False)) # sort by abs_r so that biggest r values are on top\n",
    "display(sig_corr_table[sig_corr_table.X.isin([\"mood\",\"sentiment_negative\",\"sentiment_positive\"])])\n",
    "display(pg.pairwise_corr(data=data, columns=[\"sentiment_positive\",\"sentiment_negative\"], covar=\"display_text_width\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20100349",
   "metadata": {},
   "source": [
    "# Section 3: Data Wrangling <a id=data-wrangling></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcef217",
   "metadata": {},
   "source": [
    "# Create new variable categorical variable `dom_mot` \n",
    "* that is the highest motive of that tweet.\n",
    "* this variable was created in order to predict the dominant motive of a tweet using other information about that tweet. This was then abandoned since I failed to successfully program this analysis.\n",
    "* the participant has to rate their motive above average (4,5,6 but not 1,2,3). Otherwise it is assigned NaN, hence no dominant motive.\n",
    "* Analysis using `dom_mot` did not make it to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable that holds the names of mot variables\n",
    "mots = ['motive_entertain', 'motive_expressopinion', 'motive_provoke', 'motive_savecontent', 'motive_showemotions', 'motive_connectwothers', 'motive_showachievement', 'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention', 'motive_provepoint', 'motive_causechaos', 'motive_bringattention', 'motive_influence', 'motive_surpriseothers', 'motive_informothers'] \n",
    "\n",
    "# create a variable that holds the names of sent variables\n",
    "sents =  ['sentiment_anger', 'sentiment_disgust', 'sentiment_fear',      'sentiment_anticipation', 'sentiment_joy', 'sentiment_sadness','sentiment_surprise', 'sentiment_trust', 'sentiment_negative','sentiment_positive']\n",
    "\n",
    "data[\"dom_mot\"] = data[mots].idxmax(axis=1).str[7:] # name of the motive with the highest score\n",
    "data.loc[data[mots].max(axis=1) < 4, \"dom_mot\"] = np.nan # assign NaN to dom_mot whenever a tweet has less than 4 for the max motive.\n",
    "print(data.dom_mot.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ebd93",
   "metadata": {},
   "source": [
    "# Section 4: Tweet Analysis <a id=tweet-analysis></a>\n",
    "Analysis of Tweet Sentiments, Engagement, and Motives. Investigating whether tweets of different sentiments receive higher engagement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"dom_sent\"] = data[sents].idxmax(axis=1).str[10:] # name of the sentiment with highest percentage\n",
    "data.loc[data[sents].max(axis=1) < 2, \"dom_sent\"] = np.nan # assign NaN to dom_mot whenever a tweet has less than 4 for the max motive.\n",
    "print(data.dom_sent.value_counts())\n",
    "sns.boxplot(data=data,x=\"pct_eng\",y=\"dom_sent\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4f83e",
   "metadata": {},
   "source": [
    "# Engagement percentage and popularity of a tweet by negative vs positive\n",
    "* t-test and Mann-Whitney U test comparing positive and negative tweets with regards to their popularity\n",
    "* MWU is reported in the internship report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"pct_eng\",\"favorite_count\",\"retweet_count\"]\n",
    "pd.options.display.max_columns = None\n",
    "display(data.groupby(\"dom_sent\")[feats].describe().loc[[\"negative\",\"positive\"],:])\n",
    "\n",
    "print(\"two sample independent t-test between for engagement percentage between  positive and negative tweets \")\n",
    "display(pg.ttest(x=data.loc[data.dom_sent == \"positive\",\"pct_eng\"],y=data.loc[data.dom_sent == \"negative\",\"pct_eng\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for favorite count between  positive and negative tweets \")\n",
    "display(pg.ttest(x=data.loc[data.dom_sent == \"positive\",\"favorite_count\"],y=data.loc[data.dom_sent == \"negative\",\"favorite_count\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for retweet count between  positive and negative tweets \")\n",
    "display(pg.ttest(x=data.loc[data.dom_sent == \"positive\",\"retweet_count\"],y=data.loc[data.dom_sent == \"negative\",\"retweet_count\"]))\n",
    "\n",
    "print(\"two sample independent mwu-test between for engagement percentage between  positive and negative tweets \")\n",
    "display(pg.mwu(x=data.loc[data.dom_sent == \"positive\",\"pct_eng\"],y=data.loc[data.dom_sent == \"negative\",\"pct_eng\"]))\n",
    "\n",
    "print(\"two sample independent mwu-test between for favorite count between  positive and negative tweets \")\n",
    "display(pg.mwu(x=data.loc[data.dom_sent == \"positive\",\"favorite_count\"],y=data.loc[data.dom_sent == \"negative\",\"favorite_count\"]))\n",
    "\n",
    "print(\"two sample independent mwu-test between for retweet count between  positive and negative tweets \")\n",
    "display(pg.mwu(x=data.loc[data.dom_sent == \"positive\",\"retweet_count\"],y=data.loc[data.dom_sent == \"negative\",\"retweet_count\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16457b3",
   "metadata": {},
   "source": [
    "# How does the sentiment of the tweet differ between different tweet types\n",
    "* Amount of sentiment_positive words retweet > original > (reply > quote)\n",
    "* Amount of sentiment_negative words retweet > original > (quote > reply)\n",
    "* The conclusion is that retweets tend to contain more sentimental words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tweet_type variable\n",
    "for index, row in data.iterrows():\n",
    "    if data.loc[index, \"is_retweet\"] == True:\n",
    "        data.loc[index, \"tweet_type\"] = \"retweet\"\n",
    "    elif (data.loc[index, \"is_reply\"] == True) & (data.loc[index, \"is_quote\"] == True):\n",
    "        data.loc[index, \"tweet_type\"] = \"quote_reply\"\n",
    "    elif (data.loc[index, \"is_reply\"] == False) & (data.loc[index, \"is_quote\"] == True): \n",
    "        data.loc[index, \"tweet_type\"] = \"quote\"\n",
    "    elif (data.loc[index, \"is_reply\"] == True) & (data.loc[index, \"is_quote\"] == False):\n",
    "        data.loc[index, \"tweet_type\"] = \"reply\"\n",
    "    elif (data.loc[index, \"is_reply\"] == False) & (data.loc[index, \"is_quote\"] == False) & (data.loc[index, \"is_retweet\"] == False):\n",
    "        data.loc[index, \"tweet_type\"] = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de2df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution, do ANOVA. test for equality of variance, then post-hoc test\n",
    "\n",
    "sns.violinplot(data=data.loc[data.tweet_type != \"quote_reply\",:],x=\"sentiment_positive\",y=\"tweet_type\",alpha=0.1)\n",
    "plt.xlim(-1,5)\n",
    "plt.show()\n",
    "display(data.loc[data.tweet_type != \"quote_reply\",:].groupby(\"tweet_type\").sentiment_positive.describe())\n",
    "display(pg.homoscedasticity(data=data.loc[data.tweet_type != \"quote_reply\",:],group=\"tweet_type\",dv=\"sentiment_positive\"))\n",
    "display(pg.welch_anova(data=data.loc[data.tweet_type != \"quote_reply\",:],between=\"tweet_type\",dv=\"sentiment_positive\"))\n",
    "display(pg.pairwise_gameshowell(data=data.loc[data.tweet_type != \"quote_reply\",:],between=\"tweet_type\",dv=\"sentiment_positive\").round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but this time for sentiment_negative\n",
    "data = data.loc[data.tweet_type != \"quote_reply\",:]\n",
    "sns.violinplot(data=data,x=\"sentiment_negative\",y=\"tweet_type\",alpha=0.1)\n",
    "plt.xlim(-1,5)\n",
    "plt.show()\n",
    "display(data.loc[data.tweet_type != \"quote_reply\",:].groupby(\"tweet_type\").sentiment_negative.describe())\n",
    "display(pg.homoscedasticity(data=data.loc[data.tweet_type != \"quote_reply\",:],group=\"tweet_type\",dv=\"sentiment_negative\"))\n",
    "display(pg.welch_anova(data=data.loc[data.tweet_type != \"quote_reply\",:],between=\"tweet_type\",dv=\"sentiment_negative\"))\n",
    "display(pg.pairwise_gameshowell(data=data.loc[data.tweet_type != \"quote_reply\",:],between=\"tweet_type\",dv=\"sentiment_negative\").round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea745b",
   "metadata": {},
   "source": [
    "# How does a tweet's motive differ between different tweet types?\n",
    "* visualizing different tweet types and distribution of motive scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e76b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mots = ['motive_provoke', 'motive_savecontent', 'motive_showemotions',\n",
    "       'motive_connectwothers', 'motive_showachievement',\n",
    "       'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention',\n",
    "       'motive_provepoint', 'motive_causechaos', 'motive_bringattention',\n",
    "       'motive_influence', 'motive_surpriseothers', 'motive_informothers']\n",
    "fig, axs = plt.subplots(len(mots),1,figsize=(10,10*len(mots)))\n",
    "for i, mot in enumerate(mots):\n",
    "    sns.kdeplot(data=data, x=mot, hue=\"tweet_type\",multiple=\"fill\",common_norm=False,ax=axs[i])\n",
    "    axs[i].set_xlim([1,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48852e39",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Predict dominant motive as a category label using sentiment scores. This analysis was abandoned as I could not make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62966ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dom_mot variable\n",
    "data[\"dom_mot\"] = data[mots].idxmax(axis=1).str[7:] # name of the motive with the highest score\n",
    "data.loc[data[mots].max(axis=1) < 4, \"dom_mot\"] = np.nan # assign NaN to dom_mot whenever a tweet has less than 4 for the max motive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c29a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary, replace some values and make it ready for the logistic regression\n",
    "mots = ['showemotions', 'informothers', 'gainattention', 'showattitude','connectwothers', 'bringattention', 'provoke', 'surpriseothers','savecontent', 'showachievement', 'provepoint', 'causechaos','influence', 'deceiveothers']\n",
    "mydict = dict((val, i+1) for i, val in enumerate(mots))\n",
    "#data.dom_mot =  data.dropna(subset=\"dom_mot\").dom_mot.replace(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sents =  ['sentiment_anger', 'sentiment_disgust', 'sentiment_fear',      'sentiment_anticipation', 'sentiment_joy', 'sentiment_sadness','sentiment_surprise', 'sentiment_trust', 'sentiment_negative','sentiment_positive']\n",
    "X = data.loc[data[\"dom_mot\"].notna(),sents]\n",
    "y = data.loc[data[\"dom_mot\"].notna(),\"dom_mot\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=3606,train_size=0.7)\n",
    "\n",
    "lr = LogisticRegression(max_iter=200,multi_class=\"auto\")\n",
    "lr.fit(X_train,y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b8a14",
   "metadata": {},
   "source": [
    "# Section 5: User Analysis <a id=user-analysis></a>\n",
    "Create `users` dataframe, aggregating tweets for each users for user level analysis `data` was a dataframe made of individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 10 char length nondates into a dummy date so that you can do rest of the calculations\n",
    "for i in data.index:\n",
    "    if len(data.loc[i,\"account_created_at\"]) == 10:\n",
    "        data.loc[i,\"account_created_at\"] = \"2000-01-01 00:00:00\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "last_day = datetime.strptime(\"2022-06-12 23:59:59\", \"%Y-%m-%d %H:%M:%S\") # last day in the dataset\n",
    "\n",
    "for i in data.index: # iterate over each row\n",
    "    x = datetime.strptime(data.loc[i,\"account_created_at\"], \"%Y-%m-%d %H:%M:%S\") # create a temporary variable that holds the datetime object of that row\n",
    "    data.loc[i,\"account_age\"] = (last_day - x).days # turn the datetime object into days count and store in a new col called \"account_age\"\n",
    "\n",
    "\n",
    "    \n",
    "#  Find the rows where sentiment_xxx is >2, for these rows add a new col called is_anger and set it to True\n",
    "for i_col in np.arange(52,62):\n",
    "    col_name = list(data)[i_col]\n",
    "    col_name_formatted = col_name[10:]\n",
    "    new_col_name = \"is_%s\"%col_name_formatted  # string formatting: https://www.geeksforgeeks.org/string-formatting-in-python/\n",
    "    data.loc[data[col_name] > 2, new_col_name] = True\n",
    "    data[new_col_name] = data[new_col_name].fillna(False)\n",
    "    \n",
    "# Create a new dataframe called users\n",
    "users = data.groupby(\"session\").created_at.count().to_frame(name=\"post_count\").reset_index()\n",
    "users[\"replies_sent\"] = data.groupby(\"session\").is_reply.sum().reset_index(name=\"reply_count\")[\"reply_count\"]\n",
    "users[\"retweets_sent\"] = data.groupby(\"session\").is_retweet.sum().reset_index(name=\"retweet_count\")[\"retweet_count\"]\n",
    "users[\"quotes_sent\"] = data.groupby(\"session\").is_quote.sum().reset_index(name=\"quote_count\")[\"quote_count\"]\n",
    "users.set_index(\"session\", inplace = True)\n",
    "users[\"quote_replies_sent\"] = data.loc[(data.is_reply == 1) & (data.is_quote == 1),:].groupby(\"session\").created_at.count()\n",
    "users[\"quote_replies_sent\"].fillna(0, inplace=True)\n",
    "users[\"tweets_sent\"] = users.post_count - users.replies_sent - users.quotes_sent - users.retweets_sent + users.quote_replies_sent\n",
    "users[\"mean_anger\"] = data.groupby(\"session\").sentiment_anger.mean()\n",
    "users[\"mean_disgust\"] = data.groupby(\"session\").sentiment_disgust.mean()\n",
    "users[\"mean_fear\"] = data.groupby(\"session\").sentiment_fear.mean()\n",
    "users[\"mean_anticipation\"] = data.groupby(\"session\").sentiment_anticipation.mean()\n",
    "users[\"mean_joy\"] = data.groupby(\"session\").sentiment_joy.mean()\n",
    "users[\"mean_sadness\"] = data.groupby(\"session\").sentiment_sadness.mean()\n",
    "users[\"mean_surprise\"] = data.groupby(\"session\").sentiment_surprise.mean()\n",
    "users[\"mean_trust\"] = data.groupby(\"session\").sentiment_trust.mean()\n",
    "users[\"mean_negative\"] = data.groupby(\"session\").sentiment_negative.mean()\n",
    "users[\"mean_positive\"] = data.groupby(\"session\").sentiment_positive.mean()\n",
    "users[\"account_age_days\"] = data[[\"account_age\",\"session\"]].replace(8198,np.nan).set_index(\"session\").dropna().groupby(\"session\").mean(\"account_age\")\n",
    "users[\"followers_count\"] = data.set_index(\"session\")[\"followers_count\"][~data.set_index(\"session\")[\"followers_count\"].index.duplicated()] # number of followers a user had when they sent their first tweet\n",
    "\n",
    "users[\"mean_favorite_count\"] = data.loc[data.tweet_type != \"retweet\",[\"favorite_count\",\"session\"]].groupby(\"session\").mean() # does not include RTs since they all have 0 favs. People who only sent RTs are assigned 0 to get rid of NaN\n",
    "users[\"mean_favorite_count\"].fillna(0, inplace=True) # users who only sent RTs had NaN values, they are replaced with 0\n",
    "users[\"mean_retweet_count\"] = data.loc[data.tweet_type != \"retweet\",[\"retweet_count\",\"session\"]].groupby(\"session\").mean() # does not include RTs since they all have 0 favs.  People who only sent RTs are assigned 0 to get rid of NaN\n",
    "users[\"mean_retweet_count\"].fillna(0, inplace=True) # users who only sent RTs had NaN values, they are replaced with 0\n",
    "\n",
    "users[\"popularity\"] = users[\"mean_favorite_count\"] + users[\"mean_retweet_count\"]\n",
    "users[\"engagement_percentage\"] = users[\"popularity\"] / users[\"followers_count\"] * 100 # \n",
    "users[\"engagement_percentage\"].replace(np.inf, np.nan, inplace=True) # users with zero followers now has NaN engagement score instead of inf which makes no sense\n",
    "users[\"engagement_percentage\"].fillna(0, inplace=True) # 0 follower dude got NaN. now they have 0 engagement as well\n",
    "users[\"mean_length\"] = data.groupby(\"session\").display_text_width.mean()\n",
    "#users[\"motive_causechaos\"] = data.loc[~data.motive_causechaos.isna(),[\"session\",\"motive_causechaos\"]].groupby(\"session\").mean()\n",
    "#users[\"angry_tweets\"] = data.loc[data.is_anger == 1,[\"session\",\"is_anger\"]].groupby(\"session\").count()\n",
    "\n",
    "# Create mean motive of the users' tweets\n",
    "mots = ['motive_entertain', 'motive_expressopinion', 'motive_provoke', 'motive_savecontent', 'motive_showemotions', 'motive_connectwothers', 'motive_showachievement', 'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention', 'motive_provepoint', 'motive_causechaos', 'motive_bringattention', 'motive_influence', 'motive_surpriseothers', 'motive_informothers']\n",
    "for col in mots:\n",
    "    users[col] = data.loc[~data[col].isna(),[\"session\",col]].groupby(\"session\").mean()\n",
    "\n",
    "# How many tweets the person has broken down by each emotion\n",
    "is_sent = ['is_anger', 'is_disgust', 'is_fear', 'is_anticipation', 'is_joy', 'is_sadness', 'is_surprise', 'is_trust','is_positive','is_negative']\n",
    "count_sent =  ['count_anger', 'count_disgust', 'count_fear', 'count_anticipation', 'count_joy', 'count_sadness', 'count_surprise', 'count_trust','count_positive','count_negative']\n",
    "for col in is_sent:\n",
    "    users[\"count_\" + col[3:]] = data.loc[data[col] == 1,[\"session\",col]].groupby(\"session\").count() # number of sentimental tweets per user\n",
    "users[count_sent] = users[count_sent].fillna(0) # fill NaN values with zero\n",
    "\n",
    "# automatize creation of new columns that show percentage of tweets that correspond to the given sentiment\n",
    "pct_sent = ['pct_anger', 'pct_disgust', 'pct_fear', 'pct_anticipation', 'pct_joy', 'pct_sadness', 'pct_surprise', 'pct_trust']\n",
    "for col in count_sent:\n",
    "    users[\"pct_\" + col[6:]] = users[col] / users.post_count * 100\n",
    "users[pct_sent] = users[pct_sent].fillna(0) # fill NaN values with zero\n",
    "\n",
    "# pct_highest shows how dominant the dom_sent is.\n",
    "users[\"pct_highest\"] = users[pct_sent].max(axis=1) # highest percentage is saced to a new col\n",
    "users[\"dom_sent\"] = users[pct_sent].idxmax(axis=1).str[4:] # name of the sentiment with highest percentage\n",
    "\n",
    "# Create is_xxx cols with motive_xxx more than 3\n",
    "mots = ['motive_entertain', 'motive_expressopinion', 'motive_provoke', 'motive_savecontent', 'motive_showemotions', 'motive_connectwothers', 'motive_showachievement', 'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention', 'motive_provepoint', 'motive_causechaos', 'motive_bringattention', 'motive_influence', 'motive_surpriseothers', 'motive_informothers']\n",
    "for col in mots:\n",
    "    data.loc[data[col] > 3, \"is_\" + col[7:] ] = True\n",
    "    data[\"is_\" + col[7:]].fillna(False, inplace = True) # The rest will be assigned NaN. Fix it with the following code.\n",
    "\n",
    "# Calculate count_motive for each user: number of tweets sent by the given motive.\n",
    "is_mot = ['is_entertain', 'is_expressopinion', 'is_provoke', 'is_savecontent', 'is_showemotions', 'is_connectwothers', 'is_showachievement', 'is_showattitude', 'is_deceiveothers', 'is_gainattention', 'is_provepoint', 'is_causechaos', 'is_bringattention', 'is_influence', 'is_surpriseothers','is_informothers']\n",
    "for col in is_mot:\n",
    "    users[\"count_\" + col[3:]] = data[is_mot + [\"session\"]].groupby(\"session\").sum().loc[:,col]\n",
    "\n",
    "# Create dom_mot variable that shows the most commonly occuring motive for each user\n",
    "count_mot = ['count_entertain', 'count_expressopinion', 'count_provoke', 'count_savecontent', 'count_showemotions', 'count_connectwothers', 'count_showachievement', 'count_showattitude', 'count_deceiveothers','count_gainattention', 'count_provepoint', 'count_causechaos','count_bringattention', 'count_influence', 'count_surpriseothers','count_informothers']\n",
    "users[\"dom_mot\"] = users[count_mot].idxmax(axis=1).str[6:] # name of the sentiment with highest percentage\n",
    "users.loc[users[count_mot].max(axis=1) == 0, \"dom_mot\"] = np.nan # assign NaN to dom_mot whenever there is no tweets with motivation info\n",
    "\n",
    "# Calculate pct_mot that show\n",
    "for col in count_mot:\n",
    "    users[\"pct_\" + col[6:]] = users[col] / users.post_count * 100\n",
    "#users[pct_sent] = users[pct_sent].fillna(0) # fill NaN values with zero\n",
    "pct_mot = ['pct_entertain', 'pct_expressopinion', 'pct_provoke', 'pct_savecontent', 'pct_showemotions', 'pct_connectwothers', 'pct_showachievement', 'pct_showattitude', 'pct_deceiveothers', 'pct_gainattention', 'pct_provepoint', 'pct_causechaos', 'pct_bringattention', 'pct_influence', 'pct_surpriseothers', 'pct_informothers']\n",
    "\n",
    "# Calculate pct_positive - pct_negative\n",
    "users[\"pct_mood\"] = users.pct_positive - users.pct_negative\n",
    "\n",
    "# Calculate pct_positive + pct_negative\n",
    "users[\"pct_total_pos_neg\"] = users.pct_positive + users.pct_negative\n",
    "\n",
    "users[\"account_age_days\"] = data[[\"account_age\",\"session\"]].replace(8198,np.nan).set_index(\"session\").dropna().groupby(\"session\").mean(\"account_age\")\n",
    "\n",
    "# assert that total number of tweets in the users table match the total number of tweets in the data table\n",
    "#assert users[\"tweets_sent\"].sum() == data.loc[(data.is_reply==0)&(data.is_retweet==0)&(data.is_quote==0),:].shape[0]\n",
    "#assert users[\"quote_replies_sent\"].sum() == data.loc[(data.is_reply==1)&(data.is_retweet==0)&(data.is_quote==1),:].shape[0]\n",
    "#assert users[\"replies_sent\"].sum() == data.loc[(data.is_reply==1),:].shape[0]\n",
    "#assert users[\"retweets_sent\"].sum() == data.loc[(data.is_retweet==1),:].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d06a0b",
   "metadata": {},
   "source": [
    "## Subset users with more than 10 non-retweet posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.loc[:,\"non_retweet_post_count\"] = users.post_count - users.retweets_sent\n",
    "users = users[users.non_retweet_post_count >= 10]\n",
    "\n",
    "# Calculate percentage of nonretweet posts per user\n",
    "users[\"pct_non_retweet\"] = users.non_retweet_post_count / users.post_count * 100\n",
    "\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaae9ef",
   "metadata": {},
   "source": [
    "# Section 6: Attrition Bias <a id=attrition-bias></a>\n",
    "Testing attrition bias, i.e., comparing tweets of those who filled out the survey vs did not. Not reported in the report due to space limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a57388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"two sample independent t-test between for ACCOUNT AGE vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"account_age_days\"],y=users.loc[users.motive_entertain.notna(),\"account_age_days\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for POST COUNT vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"post_count\"],y=users.loc[users.motive_entertain.notna(),\"post_count\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for FOLLOWERS COUNT vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"followers_count\"],y=users.loc[users.motive_entertain.notna(),\"followers_count\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for ENGAGEMENT PERCENTAGE vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"engagement_percentage\"],y=users.loc[users.motive_entertain.notna(),\"engagement_percentage\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for MEAN LENGTH vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"mean_length\"],y=users.loc[users.motive_entertain.notna(),\"mean_length\"]))\n",
    "\n",
    "print(\"two sample independent t-test between for MEAN LENGTH vals between responders and nonresponders\")\n",
    "display(pg.ttest(x=users.loc[users.motive_entertain.isna(),\"mean_length\"],y=users.loc[users.motive_entertain.notna(),\"mean_length\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c388ebc",
   "metadata": {},
   "source": [
    "## Some scatterplots to explore `users` by the following variables: followers_count, account_age, number_of_tweets, ratio of original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=users,y=\"followers_count\",x=\"pct_non_retweet\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a927ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=users,x=\"account_age_days\",y=\"followers_count\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=users,x=\"post_count\",y=\"followers_count\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3abf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=users,y=\"popularity\",x=\"pct_non_retweet\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b80bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=users,x=\"followers_count\",y=\"popularity\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c64ed",
   "metadata": {},
   "source": [
    "# Section 7: Clustering <a id=clustering></a>\n",
    "Clustering users based on followers_count, account_age, number_of_tweets, ratio of original tweets, mean_tweet_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dea02",
   "metadata": {},
   "source": [
    "## Data wrangling for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kmeans(data, max_k): # Create a function that applies k-means clustering with increasingly many clusters and plits an elbow plot.\n",
    "    \"\"\"Applies k-means clustering to data for 1 cluster  upto max_k clusters. Draws an elbow plot for you to \n",
    "    manually determine the appropriate number of clusters. The point that corresponds to the elbow is the right\n",
    "    amount of cluster you should have\"\"\"\n",
    "\n",
    "    n_k = []\n",
    "    inertias = []\n",
    "    \n",
    "    for k in range(1, max_k+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(data)\n",
    "        n_k.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "     \n",
    "    # elbow plot\n",
    "    fig = plt.subplots(figsize=(15,5))\n",
    "    plt.plot(n_k,inertias,\"o-\")\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.grid(True)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"followers_count\",\"account_age_days\",\"post_count\",\"pct_non_retweet\",\"popularity\",\"mean_length\"] # features I would like to use for my clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# using standard scaler to prepare features for the clustering.\n",
    "scaler = StandardScaler() \n",
    "feats_t = []\n",
    "for feat in feats:\n",
    "    feats_t.append(feat + \"_t\")\n",
    "    \n",
    "users[feats_t] = scaler.fit_transform(users[feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82054f",
   "metadata": {},
   "source": [
    "## Section 7.1: K-means clustering <a id=k-means-clustering></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576df954",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_kmeans(users[feats_t].dropna(),8)\n",
    "_=plt.title(\"Elbow plot for determining the appropriate number of clusters\")\n",
    "plt.savefig(\"/Users/ekinderdiyok/Documents/MPI/Twitter/Visualizations/elbow.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06f96a",
   "metadata": {},
   "source": [
    "## Section 7.2: t-SNE clustering <a id=t-sne></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(learning_rate=100)\n",
    "transformed = model.fit_transform(users[feats_t].dropna())\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bartlett test comparing variances\n",
    "from scipy.stats import bartlett\n",
    "stats = []\n",
    "ps = []\n",
    "for mot in mots:\n",
    "    OPs = data.loc[data.tweet_type == \"original\",mot]\n",
    "    reps_rets = data.loc[data.tweet_type == \"retweet\",mot]\n",
    "    stat, p = bartlett(OPs, reps_rets)\n",
    "    stats.append(stat)\n",
    "    ps.append(p)\n",
    "my_bartlett = pd.DataFrame(list([mots, stats, ps])).T\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "my_bartlett = my_bartlett.rename(columns={0:\"motive\",1:\"test statistics\",2:\"p-values\"})\n",
    "my_bartlett.set_index(\"motive\")\n",
    "my_bartlett.to_csv(\"/Users/ekinderdiyok/Documents/MPI/Twitter/my_bartlett_v02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.loc[data.tweet_type==\"original\",mots].describe().loc[\"std\"])\n",
    "display(data.loc[data.tweet_type==\"retweet\",mots].describe().loc[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a587c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mots = ['motive_provoke', 'motive_savecontent', 'motive_showemotions',\n",
    "       'motive_connectwothers', 'motive_showachievement',\n",
    "       'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention',\n",
    "       'motive_provepoint', 'motive_causechaos', 'motive_bringattention',\n",
    "       'motive_influence', 'motive_surpriseothers', 'motive_informothers']\n",
    "fig, axs = plt.subplots(len(mots),1,figsize=(10,10*len(mots)))\n",
    "for i, mot in enumerate(mots):\n",
    "    sns.kdeplot(data=data, x=mot, hue=\"tweet_type\",multiple=\"fill\",common_norm=False,ax=axs[i])\n",
    "    axs[i].set_xlim([1,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"tweet_type\")[mots].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4ce76",
   "metadata": {},
   "source": [
    "data_rt_ori = data.loc[data.tweet_type.isin([\"retweet\",\"original\"])]\n",
    "data_rt_ori.tweet_type.value_counts()\n",
    "data_rt_sampled = data.loc[data.tweet_type == \"retweet\"].sample(4135)\n",
    "data_ori = data.loc[data.tweet_type == \"original\"]\n",
    "data_ori.tweet_type.value_counts()\n",
    "data_ori_rt_sampled = pd.concat([data_ori, data_rt_sampled], axis=1)\n",
    "data_ori_rt_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tweet_type.value_counts()\n",
    "data_rt_sampled = data.loc[data.tweet_type == \"retweet\",:].sample(4135)\n",
    "data_og = data.loc[data.tweet_type == \"original\",:]\n",
    "data_rt_og = pd.concat([data_og,data_rt_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b8a07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mots = ['motive_provoke', 'motive_savecontent', 'motive_showemotions',\n",
    "       'motive_connectwothers', 'motive_showachievement',\n",
    "       'motive_showattitude', 'motive_deceiveothers', 'motive_gainattention',\n",
    "       'motive_provepoint', 'motive_causechaos', 'motive_bringattention',\n",
    "       'motive_influence', 'motive_surpriseothers', 'motive_informothers']\n",
    "fig, axs = plt.subplots(len(mots),1,figsize=(10,10*len(mots)))\n",
    "for i, mot in enumerate(mots):\n",
    "    sns.kdeplot(data=data_rt_og, x=mot, hue=\"tweet_type\",multiple=\"fill\",common_norm=False,ax=axs[i])\n",
    "    axs[i].set_xlim([1,6])\n",
    "    axs[i].axhline(y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3415a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "ps = []\n",
    "std_rt = []\n",
    "std_op = []\n",
    "mean_rt = []\n",
    "mean_op = []\n",
    "\n",
    "for mot in mots:\n",
    "    OPs = data.loc[data.tweet_type == \"original\",mot]\n",
    "    rts = data.loc[data.tweet_type == \"retweet\",mot]\n",
    "    #reps_rets = data.loc[data.tweet_type.isin([\"reply\" or \"retweet\"]),mot]\n",
    "    stat, p = bartlett(OPs, rts)\n",
    "    stats.append(stat)\n",
    "    ps.append(p)\n",
    "    std_op.append(data.loc[data.tweet_type==\"original\",mot].std())\n",
    "    std_rt.append(data.loc[data.tweet_type==\"retweet\",mot].std())\n",
    "    #mean_op.append(OPs.mean())\n",
    "    #mean_rt.append(rts.mean())\n",
    "\n",
    "difference = (pd.Series(std_op) - pd.Series(std_rt))/pd.Series(std_rt)*100\n",
    "my_bartlett = pd.DataFrame(list([mots, stats, ps,std_op,std_rt,difference])).T\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "my_bartlett = my_bartlett.rename(columns={0:\"motive\",1:\"test statistics\",2:\"p-values\",3:\"Original post SD\",4:\"Retweet SD\",5:\"Percentage difference SD\"})\n",
    "my_bartlett.set_index(\"motive\")\n",
    "my_bartlett.to_csv(\"/Users/ekinderdiyok/Documents/MPI/Twitter/my_bartlett_v03.csv\")\n",
    "my_bartlett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to visualize negativity bias. I decided to report the violin plot and abandoned this plot\n",
    "\n",
    "x1 = data.loc[(data.dom_sent == \"negative\") & (data.retweet_count > 0),[\"retweet_count\",\"dom_sent\"]]\n",
    "x2 = data.loc[(data.dom_sent == \"positive\") & (data.retweet_count > 0),[\"retweet_count\",\"dom_sent\"]]\n",
    "xs = pd.concat([x1,x2])\n",
    "sns.boxplot(data=xs,x=\"retweet_count\",y=\"dom_sent\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violinplot to visualize negativity bias\n",
    "\n",
    "x1 = data.loc[(data.dom_sent == \"negative\"),[\"retweet_count\",\"dom_sent\"]]\n",
    "x2 = data.loc[(data.dom_sent == \"positive\"),[\"retweet_count\",\"dom_sent\"]]\n",
    "xs = pd.concat([x1,x2])\n",
    "sns.violinplot(data=xs,x=\"retweet_count\",y=\"dom_sent\",scale=\"area\")\n",
    "plt.xscale(\"log\")\n",
    "#plt.xlim([1,100000])\n",
    "plt.xlabel(\"Retweet count\")\n",
    "plt.ylabel(\"Sentiment of the tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data.loc[(data.dom_sent == \"negative\"),[\"retweet_count\",\"dom_sent\"]]\n",
    "x2 = data.loc[(data.dom_sent == \"positive\"),[\"retweet_count\",\"dom_sent\"]]\n",
    "xs = pd.concat([x1,x2])\n",
    "#sns.kdeplot(x=x1.retweet_count)\n",
    "sns.kdeplot(data=xs, x=\"retweet_count\",hue=\"dom_sent\",multiple=\"fill\")\n",
    "plt.xscale(\"log\")\n",
    "#plt.xlim([1,100000])\n",
    "plt.xlabel(\"Retweet count\")\n",
    "#plt.ylabel(\"Sentiment of the tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61697000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF plot to visualize negativity bias. I decided to report the violin plot and abandoned this plot\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "sns.ecdfplot(data=xs, x=\"retweet_count\", hue=\"dom_sent\",ax=axs[0])\n",
    "axs[0].set_xscale(\"log\")\n",
    "#plt.xlim([30000,1000000])\n",
    "#plt.ylim([0.9,1])\n",
    "\n",
    "sns.ecdfplot(data=xs, x=\"retweet_count\", hue=\"dom_sent\",ax=axs[1])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim([30000,1000000])\n",
    "plt.ylim([0.98,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ab01a",
   "metadata": {},
   "source": [
    "# Section 8: Multiple Linear Regression <a id=multiple-linear-regression></a>\n",
    "to explain a single motive with multiple sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.dropna(subset=\"motive_entertain\")[sents]\n",
    "y = data['motive_showemotions'].dropna()\n",
    "lm = pg.linear_regression(X, y)\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2861da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_n = []\n",
    "for sent in sents:\n",
    "    sent_n.append(sent + \"_n\")\n",
    "\n",
    "for sent in sents:\n",
    "    \n",
    "    test[sent_n] = data[sent] / data.display_text_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86004786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[sent] / data.display_text_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[sents].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of the script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
